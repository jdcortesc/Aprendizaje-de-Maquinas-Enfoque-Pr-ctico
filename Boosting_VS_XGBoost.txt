Diferencias entre Gradient Boosting y XGBoost

XGBoost es una herramienta poderosa y rapida, muy utilizada en competencias de Kagle, actualmente suelen confundirse los Algortimos Gradient Boosting, Gradient Boosted Trees y XGBoost, a continuaci—n se realizar‡ una breve explicaci—n de las semejanzas y diferencias de cada uno de estos.

Gradient Boosting: Como todo algoritmo Boosting, se basa en el algoritmo de gradiente descendiente, sin embargo, en el caso del Gradient Boosting no se asume una estructura fija (distribucion) para la prediccion. Es decir, se desea encontrar los mejores parametros de la funcion que determina la prediccion y la mejor funcion. Adicionalmente, el objetivo es minimizar la funci—n de perdida, ensamblando aprendizajes simples de manera iterativa. En cada paso del proceso se reajustan los pesos de las predicciones incorrectas.

Gradient Boosted trees: Es un caso particular del Gradient Boosting, la funcion que predice, es decir los aprendizajes debiles, son arboles de decisi—n. Consecuentemente, los par‡metros de entrada del algortimo son los mismo de un arbol de decisi—n mas los pesos de cada arbol.

XGBoost: Es un Gradient Boosted Trees ajustado para ser mas eficiente, porque no se considera la perdida potencial de todos los posibles splits para crear una nueva rama. Adicionalmente se realizan procesos de regularizacion, por lo que la cantidad de hiperparametros aumenta pero el overfitting disminuye, algunos de los mas determinantes son

n_estimators: Numero de arboles a ensamblar
max_depth: Maxima profundidad de cada arbol
learningÊrate: cada peso en todos los arboles se multiplica por este valor.




