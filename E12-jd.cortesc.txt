Gradient Boosting and XGBoostGradient boosting:El algoritmo de gradiente descendiente es la base del gradient boosting, sin embargo, en el gradient boosting no se asume una estructura fija (distribucion) para la prediccion. Es decir, se desea encontrar los mejores parametros de la funcion que determina la prediccion y la mejor funcion.El objetivo es minimizar la funcion de perdida.Se desea componer un aprendizaje robusto, con base en aprendizajes debiles.2. Gradient Boosted TreesEs un caso particular del Gradient boosting, los aprendizajes debiles son arboles de decision. Los parametros son los parametros de cada arbol y los pesos de cada arbol. 3. XGBoost (and its hyperparameters)Es uno de los mas rapidos gradient boosting trees. Lo anterior porque no se considera la perdida potencial de todos los posibles splits para crear una nueva rama. Adicionalmente se realizan procesos de regularizacion, por lo que la cantidad de hiperparametros aumenta pero el overfitting disminuye. Algunos de los mas determinantes sonn_estimatorsmax_depthlearning rate: cada peso en todos los arboles se multiplica por este valorreg_alpha y reg_lambda: En conclusion,Gabriel TsengApr 13, 2018Diferencias entre Gradient Boosting y XGBoostXGBoost es una herramienta poderosa y rapida, muy utilizada en competencias de Kagle, actualmente suelen confundirse los Algortimos Gradient Boosting, Gradient Boosted Trees y XGBoost, a continuación se realizará una breve explicación de las semejanzas y diferencias de cada uno de estos.Gradient Boosting: Como todo algoritmo Boosting, se basa en el algoritmo de gradiente descendiente, sin embargo, en el caso del Gradient Boosting no se asume una estructura fija (distribucion) para la prediccion. Es decir, se desea encontrar los mejores parametros de la funcion que determina la prediccion y la mejor funcion. Adicionalmente, el objetivo es minimizar la función de perdida, ensamblando aprendizajes simples de manera iterativa. En cada paso del proceso se reajustan los pesos de las predicciones incorrectas.Gradient Boosted trees: Es un caso particular del Gradient Boosting, la funcion que predice, es decir los aprendizajes debiles, son arboles de decisión. Consecuentemente, los parámetros de entrada del algortimo son los mismo de un arbol de decisión mas los pesos de cada arbol.XGBoost: Es un Gradient Boosted Trees ajustado para ser mas eficiente, porque no se considera la perdida potencial de todos los posibles splits para crear una nueva rama. Adicionalmente se realizan procesos de regularizacion, por lo que la cantidad de hiperparametros aumenta pero el overfitting disminuye, algunos de los mas determinantes sonn_estimators: Numero de arboles a ensamblarmax_depth: Maxima profundidad de cada arbollearning rate: cada peso en todos los arboles se multiplica por este valor.